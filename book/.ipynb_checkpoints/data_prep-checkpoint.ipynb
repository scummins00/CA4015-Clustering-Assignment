{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcbd8f5",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "In the following Notebook, we will verify the integrity of our data. The data provided by 10 individual studies and centralised by {cite}`Steingroever2015`, is inherently clean and ready for use. To ensure this, we will perform the following verification steps:\n",
    "\n",
    "1. Test all datasets for any missing values.\n",
    "2. Verify that deck choice datasets do not host cells exceding a maximum value of 4 and a minimum value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40315492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9b0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n",
      "NaN value detected: False\n"
     ]
    }
   ],
   "source": [
    "#Reading in the data\n",
    "sets = []\n",
    "\n",
    "#choices\n",
    "choice_95=pd.read_csv(\"../data/choice_95.csv\")\n",
    "sets.append(choice_95)\n",
    "choice_100=pd.read_csv(\"../data/choice_100.csv\")\n",
    "sets.append(choice_100)\n",
    "choice_150=pd.read_csv(\"../data/choice_150.csv\")\n",
    "sets.append(choice_150)\n",
    "\n",
    "#Losses\n",
    "loss_95=pd.read_csv(\"../data/lo_95.csv\")\n",
    "sets.append(loss_95)\n",
    "loss_100=pd.read_csv(\"../data/lo_100.csv\")\n",
    "sets.append(loss_100)\n",
    "loss_150=pd.read_csv(\"../data/lo_150.csv\")\n",
    "sets.append(loss_150)\n",
    "#Wins\n",
    "win_95=pd.read_csv(\"../data/wi_95.csv\")\n",
    "sets.append(win_95)\n",
    "win_100=pd.read_csv(\"../data/wi_100.csv\")\n",
    "sets.append(win_100)\n",
    "win_150=pd.read_csv(\"../data/wi_150.csv\")\n",
    "sets.append(win_150)\n",
    "\n",
    "#Index\n",
    "index_95=pd.read_csv(\"../data/index_95.csv\")\n",
    "sets.append(index_95)\n",
    "index_100=pd.read_csv(\"../data/index_100.csv\")\n",
    "sets.append(index_100)\n",
    "index_150=pd.read_csv(\"../data/index_150.csv\")\n",
    "sets.append(index_150)\n",
    "\n",
    "for set in sets:\n",
    "    print(\"NaN value detected: {}\".format(set.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cf5578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min    1.0\n",
       "max    4.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's view the statitics for our choice datasets to verify min & max values\n",
    "(choice_100.T.describe()).iloc[[3,-1]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec705ac",
   "metadata": {},
   "source": [
    "**Note: This part of data exploration is simple by nature, but generates a large output. For simplicity, I have only included the verification of the small choice_100 dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bca1a",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "In section 4 of this book, we will be performing K-Means clustering based on a participant's net win and loss over the course of the game with results measured in 10% intervals of completion. That is to say, **in the case of a participant with 100 attempts, every 10 attempts will be condensed into one score.** \n",
    "\n",
    "In the case of participants with 150 attempts, every **15 consecutive attempts will be condensed into a singular value**.\n",
    "\n",
    "In the case of participants with 95 attempts, some calculation will be required to aggregate data points together and obtain a mean value. This is required as *10% of 95 is 9.5*. Clearly we cannot measure the 9.5th turn. This means we will measure **the mean of the 9th and 10th turn**.\n",
    "\n",
    "To do this, our data requires some **Feature Engineering**. We require a new dataset consisting of the scores described above *per participant*. Also, in Section 5, we will be performing the same analysis, but with a **Federated Learning** approach. This means that one large dataset will not suffice. For each of the original datasets provided we must:\n",
    "\n",
    "1. Create and fill our rolling score datasets\n",
    "2. Divide the data out into their individual surveys "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a659a",
   "metadata": {},
   "source": [
    "## Creating Rolling Dataframes\n",
    "The creation of Dataframes to hold rolling sumations of values across periods of 10 & 15 attempts for the surveys allowing 100 & 150 attempts respectfully is a painless process.\n",
    "\n",
    "However, this is not the case with the survey offering 95 attempts as 95 is an uneven number meaning it does not divide easily into equally sized portions. As a consequence of this, the processing steps for the 95 dataset are much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d141fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use pandas.DataFrame.rolling() to calculate our rolling sum\n",
    "\n",
    "rolling_win_100=(win_100.rolling(window=10, axis=1).sum()).iloc[:, range(9,100,10)]\n",
    "rolling_loss_100=(loss_100.rolling(window=10, axis=1).sum()).iloc[:, range(9,100,10)]\n",
    "\n",
    "rolling_win_150=(win_150.rolling(window=10, axis=1).sum()).iloc[:, range(14,150,15)]\n",
    "rolling_loss_150=(loss_150.rolling(window=10, axis=1).sum()).iloc[:, range(14,150,15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c2b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The rolling values for the 95 sets are more difficult as 95 is not divisible by 10\n",
    "inter_95=(win_95.rolling(window=10, axis=1).sum()).iloc[:,[9,18,27,28,37,46,47,56,65,66,75,84,85, 94]]\n",
    "\n",
    "#Finding the rolling sum for 9th column\n",
    "wins_95_col8=(win_95.rolling(window=9, axis=1).sum()).iloc[:,8]\n",
    "\n",
    "#Calculating the average of intermediate columns as new column\n",
    "Wins_9_5=(wins_95_col8+inter_95.iloc[:,0])/2\n",
    "Wins_28_5=(inter_95.iloc[:,2]+inter_95.iloc[:,3])/2\n",
    "Wins_47_5=(inter_95.iloc[:,5]+inter_95.iloc[:,6])/2\n",
    "Wins_66_5=(inter_95.iloc[:,8]+inter_95.iloc[:,9])/2\n",
    "Wins_85_5=(inter_95.iloc[:,11]+inter_95.iloc[:,12])/2\n",
    "\n",
    "#Add everything together\n",
    "inter_win_95=pd.concat([inter_95, Wins_9_5.rename(\"Wins_9_5\"), Wins_28_5.rename(\"Wins_28_5\"), \n",
    "                        Wins_47_5.rename(\"Wins_47_5\"),Wins_66_5.rename(\"Wins_66_5\"),\n",
    "                        Wins_85_5.rename(\"Wins_85_5\")], axis=1)\n",
    "\n",
    "#Reorganise columns\n",
    "cols = inter_win_95.columns.tolist()\n",
    "rolling_win_95 = inter_win_95[[cols[-5], cols[1], cols[-4], cols[4], cols[-3], cols[7], cols[-2], cols[10], cols[-1], cols[13]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f2042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we must do the same for the Losses\n",
    "#The rolling values for the 95 sets are more difficult as 95 is not divisible by 10\n",
    "inter_loss_95=(loss_95.rolling(window=10, axis=1).sum()).iloc[:,[9,18,27,28,37,46,47,56,65,66,75,84,85, 94]]\n",
    "\n",
    "#Finding the rolling sum for 9th column\n",
    "losses_95_col8=(loss_95.rolling(window=9, axis=1).sum()).iloc[:,8]\n",
    "\n",
    "#Calculating the average of intermediate columns as new column\n",
    "Losses_9_5=(losses_95_col8+inter_loss_95.iloc[:,0])/2\n",
    "Losses28_5=(inter_loss_95.iloc[:,2]+inter_loss_95.iloc[:,3])/2\n",
    "Losses47_5=(inter_loss_95.iloc[:,5]+inter_loss_95.iloc[:,6])/2\n",
    "Losses66_5=(inter_loss_95.iloc[:,8]+inter_loss_95.iloc[:,9])/2\n",
    "Losses85_5=(inter_loss_95.iloc[:,11]+inter_loss_95.iloc[:,12])/2\n",
    "\n",
    "#Add everything together\n",
    "inter_loss_95=pd.concat([inter_loss_95, Losses_9_5.rename(\"Losses_9_5\"), Losses28_5.rename(\"Losses28_5\"), \n",
    "                        Losses47_5.rename(\"Losses47_5\"),Losses66_5.rename(\"Losses66_5\"),\n",
    "                        Losses85_5.rename(\"Losses85_5\")], axis=1)\n",
    "\n",
    "#Reorganise columns\n",
    "cols = inter_loss_95.columns.tolist()\n",
    "rolling_loss_95 = inter_loss_95[[cols[-5], cols[1], cols[-4], cols[4], cols[-3], cols[7], cols[-2], cols[10], cols[-1], cols[13]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbabe8",
   "metadata": {},
   "source": [
    "## Seperate Data by Study\n",
    "We will now seperate our data by study. We can achieve this by using our `index` files which allows us to seperate our subjects row-wise. We will do the following:\n",
    "\n",
    "1. Append our index value as a new column\n",
    "2. Group our data by this new column\n",
    "3. Select each study as a subset and create a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d12ef253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List for sub sets\n",
    "finished_sets = []\n",
    "\n",
    "#List for full sets\n",
    "full_sets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332e99b",
   "metadata": {},
   "source": [
    "### Larger Rolling Datasets\n",
    "While the seperated studies will be beneficial for the federated learning approach, it is important to keep the aggregated datasets also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bc21c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wins\n",
    "full_rolling_wins_95 =rolling_win_95.reset_index(drop=True).join(index_95)\n",
    "full_rolling_wins_100 =rolling_win_100.reset_index(drop=True).join(index_100)\n",
    "full_rolling_wins_150 =rolling_win_150.reset_index(drop=True).join(index_150)\n",
    "full_sets.append(full_rolling_wins_95)\n",
    "full_sets.append(full_rolling_wins_100)\n",
    "full_sets.append(full_rolling_wins_150)\n",
    "\n",
    "#losses\n",
    "full_rolling_losses_95=rolling_loss_95.reset_index(drop=True).join(index_95)\n",
    "full_rolling_losses_100=rolling_loss_100.reset_index(drop=True).join(index_100)\n",
    "full_rolling_losses_150=rolling_loss_150.reset_index(drop=True).join(index_150)\n",
    "full_sets.append(full_rolling_losses_95)\n",
    "full_sets.append(full_rolling_losses_100)\n",
    "full_sets.append(full_rolling_losses_150)\n",
    "\n",
    "#Choices\n",
    "full_rolling_choices_95=choice_95.reset_index(drop=True).join(index_95)\n",
    "full_rolling_choices_100=choice_100.reset_index(drop=True).join(index_100)\n",
    "full_rolling_choices_150=choice_150.reset_index(drop=True).join(index_150)\n",
    "full_sets.append(full_rolling_choices_95)\n",
    "full_sets.append(full_rolling_choices_100)\n",
    "full_sets.append(full_rolling_choices_150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995752c6",
   "metadata": {},
   "source": [
    "### The 95 Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1e22fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wins\n",
    "Fridberg_rolling_wins_95=rolling_win_95.reset_index(drop=True).join(index_95)\n",
    "finished_sets.append(Fridberg_rolling_wins_95)\n",
    "\n",
    "#Losses\n",
    "Fridberg_rolling_losses_95=rolling_loss_95.reset_index(drop=True).join(index_95)\n",
    "finished_sets.append(Fridberg_rolling_losses_95)\n",
    "\n",
    "#Choices\n",
    "Fridberg_choices_95=choice_95.reset_index(drop=True).join(index_95)\n",
    "finished_sets.append(Fridberg_choices_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690368ec",
   "metadata": {},
   "source": [
    "### The 100 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "275b7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wins\n",
    "grouped_wins_100 = rolling_win_100.reset_index(drop=True).join(index_100).groupby(\"Study\")\n",
    "\n",
    "Horstmann_rolling_wins_100=grouped_wins_100.get_group(\"Horstmann\")\n",
    "finished_sets.append(Horstmann_rolling_wins_100)\n",
    "\n",
    "Kjome_rolling_wins_100=grouped_wins_100.get_group(\"Kjome\")\n",
    "finished_sets.append(Kjome_rolling_wins_100)\n",
    "\n",
    "Maia_rolling_wins_100=grouped_wins_100.get_group(\"Maia\")\n",
    "finished_sets.append(Maia_rolling_wins_100)\n",
    "\n",
    "SteingroverInPrep_rolling_wins_100=grouped_wins_100.get_group(\"SteingroverInPrep\")\n",
    "finished_sets.append(SteingroverInPrep_rolling_wins_100)\n",
    "\n",
    "Premkumar_rolling_wins_100=grouped_wins_100.get_group(\"Premkumar\")\n",
    "finished_sets.append(Premkumar_rolling_wins_100)\n",
    "\n",
    "Wood_rolling_wins_100=grouped_wins_100.get_group(\"Wood\")\n",
    "finished_sets.append(Wood_rolling_wins_100)\n",
    "\n",
    "Worthy_rolling_wins_100=grouped_wins_100.get_group(\"Worthy\")\n",
    "finished_sets.append(Worthy_rolling_wins_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2408de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Losses\n",
    "grouped_losses_100 = rolling_loss_100.reset_index(drop=True).join(index_100).groupby(\"Study\")\n",
    "\n",
    "Horstmann_rolling_losses_100=grouped_losses_100.get_group(\"Horstmann\")\n",
    "finished_sets.append(Horstmann_rolling_losses_100)\n",
    "\n",
    "Kjome_rolling_losses_100=grouped_losses_100.get_group(\"Kjome\")\n",
    "finished_sets.append(Kjome_rolling_losses_100)\n",
    "\n",
    "Maia_rolling_losses_100=grouped_losses_100.get_group(\"Maia\")\n",
    "finished_sets.append(Maia_rolling_losses_100)\n",
    "\n",
    "SteingroverInPrep_rolling_losses_100=grouped_losses_100.get_group(\"SteingroverInPrep\")\n",
    "finished_sets.append(SteingroverInPrep_rolling_losses_100)\n",
    "\n",
    "Premkumar_rolling_losses_100=grouped_losses_100.get_group(\"Premkumar\")\n",
    "finished_sets.append(Premkumar_rolling_losses_100)\n",
    "\n",
    "Wood_rolling_losses_100=grouped_losses_100.get_group(\"Wood\")\n",
    "finished_sets.append(Wood_rolling_losses_100)\n",
    "\n",
    "Worthy_rolling_losses_100=grouped_losses_100.get_group(\"Worthy\")\n",
    "finished_sets.append(Worthy_rolling_losses_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fb2ef63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choices\n",
    "grouped_choices_100 = choice_100.reset_index(drop=True).join(index_100).groupby(\"Study\")\n",
    "\n",
    "Horstmann_choices_100=grouped_choices_100.get_group(\"Horstmann\")\n",
    "finished_sets.append(Horstmann_choices_100)\n",
    "\n",
    "Kjome_choices_100=grouped_choices_100.get_group(\"Kjome\")\n",
    "finished_sets.append(Kjome_choices_100)\n",
    "\n",
    "Maia_choices_100=grouped_choices_100.get_group(\"Maia\")\n",
    "finished_sets.append(Maia_choices_100)\n",
    "\n",
    "SteingroverInPrep_choices_100=grouped_choices_100.get_group(\"SteingroverInPrep\")\n",
    "finished_sets.append(SteingroverInPrep_choices_100)\n",
    "\n",
    "Premkuma_choices_100=grouped_choices_100.get_group(\"Premkumar\")\n",
    "finished_sets.append(Premkuma_choices_100)\n",
    "\n",
    "Wood_choices_100=grouped_choices_100.get_group(\"Wood\")\n",
    "finished_sets.append(Wood_choices_100)\n",
    "\n",
    "Worthy_choices_100=grouped_choices_100.get_group(\"Worthy\")\n",
    "finished_sets.append(Worthy_choices_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877f284",
   "metadata": {},
   "source": [
    "### The 150 Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a606e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wins\n",
    "grouped_wins_150 = rolling_win_150.reset_index(drop=True).join(index_150).groupby(\"Study\")\n",
    "\n",
    "Steingroever2011_rolling_wins_150=grouped_wins_150.get_group(\"Steingroever2011\")\n",
    "finished_sets.append(Steingroever2011_rolling_wins_150)\n",
    "Wetzels_rolling_wins_150=grouped_wins_150.get_group(\"Wetzels\")\n",
    "finished_sets.append(Wetzels_rolling_wins_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fbe07c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Losses\n",
    "grouped_losses_150 = rolling_loss_150.reset_index(drop=True).join(index_150).groupby(\"Study\")\n",
    "\n",
    "Steingroever2011_rolling_losses_150=grouped_losses_150.get_group(\"Steingroever2011\")\n",
    "finished_sets.append(Steingroever2011_rolling_losses_150)\n",
    "Wetzels_rolling_losses_150=grouped_losses_150.get_group(\"Wetzels\")\n",
    "finished_sets.append(Wetzels_rolling_losses_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80ee13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choices\n",
    "grouped_choices_150 = choice_150.reset_index(drop=True).join(index_150).groupby(\"Study\")\n",
    "\n",
    "Steingroever2011_choices_150=grouped_choices_150.get_group(\"Steingroever2011\")\n",
    "finished_sets.append(Steingroever2011_choices_150)\n",
    "Wetzels_choices_150=grouped_choices_150.get_group(\"Wetzels\")\n",
    "finished_sets.append(Wetzels_choices_150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf1821",
   "metadata": {},
   "source": [
    "### Writing Out Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47cf78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for writing out datasets\n",
    "for s in finished_sets:\n",
    "    s.to_csv(f'../data/cleaned/{s.Study.unique()[0]}_rolling_{s.columns[0].split(\"_\")[0]}_{s.columns[-3].split(\"_\")[-1]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f330af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing out full datasets\n",
    "for s in full_sets:\n",
    "    s.to_csv(f'../data/cleaned/full_{s.columns[0].split(\"_\")[0]}_{s.columns[-3].split(\"_\")[-1]}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e865f23",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have now created a total of 39 datasets which are stored in a folder called `cleaned`. These sets consist of 9 full sets describing the amounts participants made and lost over 10% intervals. The other 30 sets are subsets of the larger 9 sets seperated by study.\n",
    "\n",
    "We will use the larger sets in Section 4 of this book for K-Means Clustering and analysis. The subsets will then be used in Section 5 as part of a Federated Learning approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
